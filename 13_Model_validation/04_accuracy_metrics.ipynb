{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858041d9-2206-45ba-8f95-8f232573458e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b2cacc-f013-4ec2-b178-cd087c8c22e2",
   "metadata": {},
   "source": [
    "## Metrics for regression models\n",
    "\n",
    "Regression models are built to analyse and predict on continuous variables.\n",
    "\n",
    "Mean Absolute Error (MAE) is a simple and intuitive metric for the absolute difference between observations and preditions. \n",
    "MAE is not sensitive to outliers.\n",
    "\n",
    "MSE (Mean Squared Error)\n",
    "\n",
    "- most widely used regression metric\n",
    "- allows larger errors to have a larger impact on the accuracy of the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef193b09-36b4-45b2-ab0e-ff7733d2f45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code not to be run as we don't have the supporting dataset loaded\n",
    "# it is for illustration purposes only\n",
    "\n",
    "# calculating MAE manually\n",
    "rfr = RandomForestRegressor(n_estimators = 500,\n",
    "                            random_state = 1111)\n",
    "\n",
    "rfr.fit(X_train, y_train)\n",
    "test_predictions = rfr.predict(X_test)\n",
    "sum(abs(y_test - test_predictions))/len(test_predictions)\n",
    "\n",
    "# calculating MAE using sklearn metrics\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(y_test, test_predictions)\n",
    "\n",
    "\n",
    "# calculating MSE manually\n",
    "sum(abs(y_test - test_predictions)**2)/len(test_predictions)\n",
    "\n",
    "# calculating MSE using sklearn metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y_test, test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f291e3-5fd6-4a47-99ca-2c7040faded1",
   "metadata": {},
   "source": [
    "## Accuracy, Precision, Recall (also called sensitivity), Specificity, F1 score, and others.\n",
    "\n",
    "We can use the scikit learn confusion matrix function to build a confusion matrix from which to calculate the various metrics.\n",
    "\n",
    "### Accuracy\n",
    "Represents the overall ability of a model to correctly predict the correct classification.\n",
    "(TN + TP) / (TN + TP + FN + FP)\n",
    "\n",
    "### Precision\n",
    "This is the number of true positives out of all predicted positive values.\n",
    "(TP) / (TP + FP)\n",
    "\n",
    "Precision is used when we don't want to overpredict positive values. If it costs $2000 to fly in someone for an interview, a company may only want to fly in someone who is a good chance of actually joining the company. This could be good for fraud detection.\n",
    "\n",
    "### Recall\n",
    "This metric is about finding all positive values.\n",
    "(TP)/(TP + FN)\n",
    "\n",
    "Recall is used when we can't afford to miss any positive values. Even if a person has a small chance of having cancer, we might want to administer additional tests. This could also be a good choice of metric when we have regulatory or compliance cases where the cost of missing a positive could be quite damaging."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
